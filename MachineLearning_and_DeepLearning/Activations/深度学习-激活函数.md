# [深度学习: 激活函数 (Activation Functions)](https://blog.csdn.net/JNingWei/article/details/79210904)

## 1. Introduction
* 激活函数（activation function）层又称 非线性映射 (non-linearity mapping) 层，作用是 增加整个网络的非线性（即 表达能力 或 抽象能力）。
* 深度学习之所以拥有 强大的表示能力 ，法门便在于 激活函数 的 非线性 。
* 然而物极必反。由于 非线性设计 所带来的一系列 副作用（如 期望均值不为0、死区），迫使炼丹师们设计出种类繁多的激活函数来 约束 非线性 的 合理范围 。
* 由于激活函数接在bn之后，所以激活函数的输入被限制在了 (-1, 1) 之间。因此，即使是relu这种简易的激活函数，也能很好地发挥作用。

## 2. 激活函数类型
激活函数中，常用的有Sigmoid、tanh(x)、Relu、Relu6、Leaky Relu、参数化Relu、随机化Relu、ELU。        
其中，最经典的莫过于 Sigmoid函数 和 Relu函数 。

### 2.1 Sigmoid
Sigmoid函数，即著名的 Logistic 函数。
被用作神经网络的阈值函数，将变量映射到 (0，1) 之间

### 2.2 tanh

### 2.3 Relu

### 2.4 Relu6

### 2.5 LeakyRelu

### 2.6 参数化Relu

### 2.7 随机化Relu

### 2.8 ELU




